#RL

## Traditional RL

- **Deep Reinforcement Learning from Human Preferences**, NeurIPS-2017 [[Paper](https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)]
- **Proximal Policy Optimization Algorithms**, arXiv-2017.08 [[Paper](http://arxiv.org/abs/1707.06347)]
- **Thinking Fast and Slow with Deep Learning and Tree Search**, NeurIPS-2017 [[Paper](https://papers.nips.cc/paper_files/paper/2017/hash/d8e1344e27a5b08cdfd5d027d9b8d6de-Abstract.html)]
- **CPPO: Continual Learning for Reinforcement Learning with Human Feedback**, ICLR-2024 [[Paper](https://openreview.net/forum?id=86zAUE80pP)]
- **Improving Generalization of Alignment with Human Preferences through Group Invariant Learning**, ICLR-2024 [[Paper](https://openreview.net/forum?id=fwCoLe3TAX)]
- **Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment**, arXiv-2023.10 [[Paper](https://arxiv.org/abs/2310.00212)]
- **ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models**, arXiv-2023.12 [[Paper](https://arxiv.org/abs/2310.10505)] [[GitHub](https://github.com/liziniu/ReMax)]

## RLHF for LLMs

### Reward modeling

- **A General Language Assistant as a Laboratory for Alignment**, arXiv-2021.12 [[Paper](http://arxiv.org/abs/2112.00861)]
- **Adversarial Preference Optimization**, arXiv-2024.02 [[Paper](https://arxiv.org/abs/2311.08045)] [[GitHub](https://github.com/Linear95/APO)]
- **SALMON: Self-Alignment with Principle-Following Reward Models**, ICLR-2024 [[Paper](https://openreview.net/forum?id=xJbsmB8UMx)] [[GitHub](https://github.com/IBM/SALMON)]
- **UltraFeedback: Boosting Language Models with High-quality Feedback**, arXiv-2023.10 [[Paper](https://arxiv.org/abs/2310.01377)] [[GitHub](https://github.com/OpenBMB/UltraFeedback)]
- **Tool-Augmented Reward Modeling**, ICLR-2024 [[Paper](https://openreview.net/forum?id=d94x0gWTUX)] [[GitHub](https://github.com/ernie-research/Tool-Augmented-Reward-Model)]
- **Let's Verify Step by Step**, ICLR-2024 [[Paper](https://openreview.net/forum?id=v8L0pN6EOi)]
- **BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset**, NeurIPS-2023 [[Paper](https://papers.nips.cc/paper_files/paper/2023/hash/4dbb61cb68671edc4ca3712d70083b9f-Abstract-Datasets_and_Benchmarks.html)] [[GitHub](https://github.com/PKU-Alignment/beavertails)] [[Feedback Dataset](https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF)]

### Basic RLHF

- **Training language models to follow instructions with human feedback**, NeurIPS-2022 [[Paper](https://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html)]
- **Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback**, arXiv-2022.04 [[Paper](http://arxiv.org/abs/2204.05862)] [[Feedback Dataset (GitHub)](https://github.com/anthropics/hh-rlhf)] [[Feedback Dataset (HF)](https://huggingface.co/datasets/Anthropic/hh-rlhf)]
- **Safe RLHF: Safe Reinforcement Learning from Human Feedback**, ICLR-2024 [[Paper](https://openreview.net/forum?id=TyFrPOKYXw)] [[GitHub](https://github.com/PKU-Alignment/safe-rlhf)]
- **Confronting Reward Model Overoptimization with Constrained RLHF**, ICLR-2024 [[Paper](https://openreview.net/forum?id=gkfUvn0fLU)] [[GitHub](https://github.com/tedmoskovitz/ConstrainedRL4LMs)]
- **WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct**, arXiv-2023.08 [[Paper](https://arxiv.org/abs/2308.09583)]

### RLAIF

- **Constitutional AI: Harmlessness from AI Feedback**, arXiv-2022.12 [[Paper](https://arxiv.org/abs/2212.08073)] [[GitHub](https://github.com/anthropics/ConstitutionalHarmlessnessPaper)]
- **RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback**, arXiv-2023.12 [[Paper](https://arxiv.org/abs/2309.00267)]

### RLHF-based non-RL fine-tuning

- **Direct Preference Optimization: Your Language Model is Secretly a Reward Model**, NeurIPS-2023 [[Paper](https://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html)] [[GitHub](https://github.com/eric-mitchell/direct-preference-optimization)]
- **A General Theoretical Paradigm to Understand Learning from Human Preferences**, arXiv-2023.11 [[Paper](https://arxiv.org/abs/2310.12036)]
- **Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints**, ICLR-2024 [[Paper](https://openreview.net/forum?id=2cRzmWXK9N)]
- **Statistical Rejection Sampling Improves Preference Optimization**, ICLR-2024 [[Paper](https://openreview.net/forum?id=xbjSwwrQOe)]

## RLHF for downstream tasks

### Summarization

- **Learning to summarize with human feedback**, NeurIPS-2020 [[Paper](https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html)] [[GitHub](https://github.com/openai/summarize-from-feedback)] [[Feedback Dataset](https://huggingface.co/datasets/openai/summarize_from_feedback)]

### Multilingual reasoning

- **MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization**, arXiv-2024.02 [[Paper](https://arxiv.org/abs/2401.06838)] [[GitHub](https://github.com/NJUNLP/MAPO)]

### ......

