#RL

## Traditional RL

- **Deep Reinforcement Learning from Human Preferences**, NeurIPS-2017 [[Paper](https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)]
- **Proximal Policy Optimization Algorithms**, arXiv-2017.08 [[Paper](http://arxiv.org/abs/1707.06347)]
- **Thinking Fast and Slow with Deep Learning and Tree Search**, NeurIPS-2017 [[Paper](https://papers.nips.cc/paper_files/paper/2017/hash/d8e1344e27a5b08cdfd5d027d9b8d6de-Abstract.html)]
- **CPPO: Continual Learning for Reinforcement Learning with Human Feedback**, ICLR-2024 [[Paper](https://openreview.net/forum?id=86zAUE80pP)]
- **Improving Generalization of Alignment with Human Preferences through Group Invariant Learning**, ICLR-2024 [[Paper](https://openreview.net/forum?id=fwCoLe3TAX)]
- **Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment**, arXiv-2023.10 [[Paper](https://arxiv.org/abs/2310.00212)]
- **ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models**, arXiv-2023.12 [[Paper](https://arxiv.org/abs/2310.10505)]

## RLHF for LLMs

### Reward modeling

- **A General Language Assistant as a Laboratory for Alignment**, arXiv-2021.12 [[Paper](http://arxiv.org/abs/2112.00861)]

### Basic RLHF

- **Training language models to follow instructions with human feedback**, NeurIPS-2022 [[Paper](https://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html)]
- **Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback**, arXiv-2022.04 [[Paper](http://arxiv.org/abs/2204.05862)] [[Feedback Dataset (GitHub)](https://github.com/anthropics/hh-rlhf)] [[Feedback Dataset (HF)](https://huggingface.co/datasets/Anthropic/hh-rlhf)]

### RLHF-based non-RL fine-tuning

- **Direct Preference Optimization: Your Language Model is Secretly a Reward Model**, NeurIPS-2023 [[Paper](https://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html)] [[GitHub](https://github.com/eric-mitchell/direct-preference-optimization)]

## RLHF for downstream tasks

### Summarization

- **Learning to summarize with human feedback**, NeurIPS-2020 [[Paper](https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html)]

### ......

